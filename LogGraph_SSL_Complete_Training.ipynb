{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e91eb5",
   "metadata": {},
   "source": [
    "# LogGraph-SSL Complete Training Notebook üöÄ\n",
    "\n",
    "## High-Performance HDFS Anomaly Detection with Self-Supervised Learning\n",
    "\n",
    "This notebook provides a complete implementation of LogGraph-SSL for anomaly detection in HDFS logs using Graph Neural Networks with Self-Supervised Learning tasks.\n",
    "\n",
    "### Features:\n",
    "- üî• **RTX 4090 Optimized**: Full GPU acceleration with CUDA 11.8\n",
    "- üß† **Multiple GNN Architectures**: GCN, GAT, GraphSAGE encoders\n",
    "- üéØ **SSL Tasks**: Node masking, edge prediction, contrastive learning\n",
    "- üìä **Real-time Monitoring**: Training metrics and GPU utilization\n",
    "- üõ°Ô∏è **Robust Architecture**: Memory efficient with gradient checkpointing\n",
    "\n",
    "### Training Pipeline:\n",
    "1. Environment setup and validation\n",
    "2. Data preprocessing and graph construction\n",
    "3. Model initialization and configuration\n",
    "4. Self-supervised pre-training\n",
    "5. Supervised fine-tuning for anomaly detection\n",
    "6. Comprehensive evaluation and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52699a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Environment Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Core data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# PyTorch and GPU libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# PyTorch Geometric for GNNs\n",
    "import torch_geometric\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import to_networkx, add_self_loops\n",
    "\n",
    "# Additional utilities\n",
    "import networkx as nx\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import psutil\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üî• Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    print(f\"üîß CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"‚ö° PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"üìä PyTorch Geometric Version: {torch_geometric.__version__}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CUDA not available. Using CPU.\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f23522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing and Graph Building Utilities\n",
    "class HDFSDataProcessor:\n",
    "    \"\"\"Comprehensive HDFS log data processor for graph construction\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=5000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        self.template_to_id = {}\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        self.graphs = []\n",
    "        \n",
    "    def build_vocabulary(self, log_data):\n",
    "        \"\"\"Build vocabulary from log templates\"\"\"\n",
    "        print(\"üîß Building vocabulary...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for line in log_data:\n",
    "            tokens = line.strip().split()\n",
    "            all_tokens.extend(tokens)\n",
    "        \n",
    "        # Count token frequencies\n",
    "        token_counts = Counter(all_tokens)\n",
    "        \n",
    "        # Create vocabulary with most frequent tokens\n",
    "        vocab_tokens = [token for token, count in token_counts.most_common(self.vocab_size - 2)]\n",
    "        \n",
    "        # Add special tokens\n",
    "        self.token_to_id = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.id_to_token = {0: '<PAD>', 1: '<UNK>'}\n",
    "        \n",
    "        for i, token in enumerate(vocab_tokens):\n",
    "            self.token_to_id[token] = i + 2\n",
    "            self.id_to_token[i + 2] = token\n",
    "            \n",
    "        print(f\"‚úÖ Vocabulary built with {len(self.token_to_id)} tokens\")\n",
    "        return self.token_to_id\n",
    "    \n",
    "    def tokenize_sequence(self, sequence):\n",
    "        \"\"\"Convert log sequence to token IDs\"\"\"\n",
    "        tokens = sequence.strip().split()\n",
    "        token_ids = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.token_to_id:\n",
    "                token_ids.append(self.token_to_id[token])\n",
    "            else:\n",
    "                token_ids.append(self.token_to_id['<UNK>'])\n",
    "                \n",
    "        return token_ids\n",
    "    \n",
    "    def build_sequence_graph(self, token_ids, max_distance=3):\n",
    "        \"\"\"Build graph from token sequence with positional and semantic edges\"\"\"\n",
    "        num_nodes = len(token_ids)\n",
    "        if num_nodes == 0:\n",
    "            return None\n",
    "            \n",
    "        # Node features (token embeddings will be learned)\n",
    "        x = torch.tensor(token_ids, dtype=torch.long).unsqueeze(1)\n",
    "        \n",
    "        # Edge construction\n",
    "        edge_indices = []\n",
    "        edge_attrs = []\n",
    "        \n",
    "        # Sequential edges (next token relationships)\n",
    "        for i in range(num_nodes - 1):\n",
    "            edge_indices.extend([[i, i+1], [i+1, i]])  # bidirectional\n",
    "            edge_attrs.extend([1, 1])  # edge type 1: sequential\n",
    "        \n",
    "        # Positional edges (skip connections)\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 2, min(i + max_distance + 1, num_nodes)):\n",
    "                edge_indices.extend([[i, j], [j, i]])\n",
    "                edge_attrs.extend([2, 2])  # edge type 2: positional\n",
    "        \n",
    "        # Token similarity edges (same tokens)\n",
    "        for i in range(num_nodes):\n",
    "            for j in range(i + 1, num_nodes):\n",
    "                if token_ids[i] == token_ids[j] and token_ids[i] != 0:  # not padding\n",
    "                    edge_indices.extend([[i, j], [j, i]])\n",
    "                    edge_attrs.extend([3, 3])  # edge type 3: token similarity\n",
    "        \n",
    "        if not edge_indices:\n",
    "            # Create self-loops if no edges\n",
    "            edge_indices = [[i, i] for i in range(num_nodes)]\n",
    "            edge_attrs = [0] * num_nodes  # edge type 0: self-loop\n",
    "        \n",
    "        edge_index = torch.tensor(edge_indices, dtype=torch.long).t()\n",
    "        edge_attr = torch.tensor(edge_attrs, dtype=torch.long)\n",
    "        \n",
    "        # Create PyTorch Geometric data object\n",
    "        graph = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        graph.num_nodes = num_nodes\n",
    "        \n",
    "        return graph\n",
    "    \n",
    "    def load_hdfs_data(self, log_file, label_file=None):\n",
    "        \"\"\"Load HDFS log data and labels\"\"\"\n",
    "        print(f\"üìÇ Loading data from {log_file}\")\n",
    "        \n",
    "        # Load log sequences\n",
    "        with open(log_file, 'r') as f:\n",
    "            log_data = f.readlines()\n",
    "        \n",
    "        # Load labels if provided\n",
    "        labels = None\n",
    "        if label_file and os.path.exists(label_file):\n",
    "            with open(label_file, 'r') as f:\n",
    "                labels = [int(line.strip()) for line in f.readlines()]\n",
    "            print(f\"üìä Loaded {len(labels)} labels\")\n",
    "        \n",
    "        # Build vocabulary\n",
    "        self.build_vocabulary(log_data)\n",
    "        \n",
    "        # Process sequences and build graphs\n",
    "        print(\"üîÑ Processing sequences and building graphs...\")\n",
    "        self.sequences = []\n",
    "        self.graphs = []\n",
    "        \n",
    "        for i, line in enumerate(tqdm(log_data, desc=\"Building graphs\")):\n",
    "            token_ids = self.tokenize_sequence(line)\n",
    "            if len(token_ids) > 0:  # Skip empty sequences\n",
    "                graph = self.build_sequence_graph(token_ids)\n",
    "                if graph is not None:\n",
    "                    self.sequences.append(token_ids)\n",
    "                    self.graphs.append(graph)\n",
    "        \n",
    "        # Set labels\n",
    "        if labels:\n",
    "            self.labels = labels[:len(self.graphs)]  # Match graph count\n",
    "        else:\n",
    "            self.labels = [0] * len(self.graphs)  # Default to normal\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(self.graphs)} sequences into graphs\")\n",
    "        print(f\"üìà Average nodes per graph: {np.mean([g.num_nodes for g in self.graphs]):.1f}\")\n",
    "        \n",
    "        return self.graphs, self.labels\n",
    "\n",
    "# Initialize data processor\n",
    "data_processor = HDFSDataProcessor(vocab_size=5000)\n",
    "print(\"‚úÖ Data processor initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83277f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete GNN Model Architectures\n",
    "\n",
    "class GCNEncoder(nn.Module):\n",
    "    \"\"\"Graph Convolutional Network encoder\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=3, dropout=0.1):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # GCN layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GCNConv(embedding_dim, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm_layers = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Get embeddings for tokens\n",
    "        if x.dim() == 2 and x.size(1) == 1:\n",
    "            x = x.squeeze(1)  # Remove extra dimension\n",
    "        \n",
    "        h = self.embedding(x)  # Shape: [num_nodes, embedding_dim]\n",
    "        \n",
    "        # Apply GCN layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            h = conv(h, edge_index)\n",
    "            h = self.norm_layers[i](h)\n",
    "            h = F.relu(h)\n",
    "            h = self.dropout(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class GATEncoder(nn.Module):\n",
    "    \"\"\"Graph Attention Network encoder\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=3, heads=4, dropout=0.1):\n",
    "        super(GATEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.heads = heads\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # GAT layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(GATConv(embedding_dim, hidden_dim // heads, heads=heads, dropout=dropout))\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(GATConv(hidden_dim, hidden_dim // heads, heads=heads, dropout=dropout))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm_layers = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Get embeddings for tokens\n",
    "        if x.dim() == 2 and x.size(1) == 1:\n",
    "            x = x.squeeze(1)\n",
    "        \n",
    "        h = self.embedding(x)\n",
    "        \n",
    "        # Apply GAT layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            h = conv(h, edge_index)\n",
    "            h = self.norm_layers[i](h)\n",
    "            h = F.relu(h)\n",
    "            h = self.dropout(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class GraphSAGEEncoder(nn.Module):\n",
    "    \"\"\"GraphSAGE encoder\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=3, dropout=0.1):\n",
    "        super(GraphSAGEEncoder, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Token embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # SAGE layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(SAGEConv(embedding_dim, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm_layers = nn.ModuleList([nn.LayerNorm(hidden_dim) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        # Get embeddings for tokens\n",
    "        if x.dim() == 2 and x.size(1) == 1:\n",
    "            x = x.squeeze(1)\n",
    "        \n",
    "        h = self.embedding(x)\n",
    "        \n",
    "        # Apply SAGE layers\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            h = conv(h, edge_index)\n",
    "            h = self.norm_layers[i](h)\n",
    "            h = F.relu(h)\n",
    "            h = self.dropout(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "class AnomalyDetectionHead(nn.Module):\n",
    "    \"\"\"Anomaly detection classification head\"\"\"\n",
    "    def __init__(self, hidden_dim, num_classes=2, dropout=0.3):\n",
    "        super(AnomalyDetectionHead, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Graph-level representation layers\n",
    "        self.global_pool = global_mean_pool\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 4, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, node_embeddings, batch=None):\n",
    "        # Global pooling to get graph-level representation\n",
    "        if batch is None:\n",
    "            # Single graph case\n",
    "            graph_embedding = torch.mean(node_embeddings, dim=0, keepdim=True)\n",
    "        else:\n",
    "            # Batch case\n",
    "            graph_embedding = self.global_pool(node_embeddings, batch)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(graph_embedding)\n",
    "        return logits\n",
    "\n",
    "class SSLTaskManager(nn.Module):\n",
    "    \"\"\"Self-supervised learning task manager\"\"\"\n",
    "    def __init__(self, hidden_dim, vocab_size):\n",
    "        super(SSLTaskManager, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Node masking task (token prediction)\n",
    "        self.token_predictor = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Edge prediction task\n",
    "        self.edge_predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Contrastive learning projection head\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        )\n",
    "        \n",
    "    def node_masking_loss(self, node_embeddings, original_tokens, masked_indices):\n",
    "        \"\"\"Compute node masking loss\"\"\"\n",
    "        if len(masked_indices) == 0:\n",
    "            return torch.tensor(0.0, device=node_embeddings.device)\n",
    "        \n",
    "        # Predict masked tokens\n",
    "        masked_embeddings = node_embeddings[masked_indices]\n",
    "        token_logits = self.token_predictor(masked_embeddings)\n",
    "        \n",
    "        # Get true tokens\n",
    "        true_tokens = original_tokens[masked_indices]\n",
    "        \n",
    "        # Compute cross-entropy loss\n",
    "        loss = F.cross_entropy(token_logits, true_tokens)\n",
    "        return loss\n",
    "    \n",
    "    def edge_prediction_loss(self, node_embeddings, edge_index, neg_edge_index):\n",
    "        \"\"\"Compute edge prediction loss\"\"\"\n",
    "        # Positive edges\n",
    "        pos_edge_embeddings = torch.cat([\n",
    "            node_embeddings[edge_index[0]], \n",
    "            node_embeddings[edge_index[1]]\n",
    "        ], dim=1)\n",
    "        pos_scores = self.edge_predictor(pos_edge_embeddings).squeeze()\n",
    "        \n",
    "        # Negative edges\n",
    "        neg_edge_embeddings = torch.cat([\n",
    "            node_embeddings[neg_edge_index[0]], \n",
    "            node_embeddings[neg_edge_index[1]]\n",
    "        ], dim=1)\n",
    "        neg_scores = self.edge_predictor(neg_edge_embeddings).squeeze()\n",
    "        \n",
    "        # Binary classification loss\n",
    "        pos_loss = F.binary_cross_entropy(pos_scores, torch.ones_like(pos_scores))\n",
    "        neg_loss = F.binary_cross_entropy(neg_scores, torch.zeros_like(neg_scores))\n",
    "        \n",
    "        return (pos_loss + neg_loss) / 2\n",
    "    \n",
    "    def contrastive_loss(self, embeddings1, embeddings2, temperature=0.1):\n",
    "        \"\"\"Compute contrastive loss\"\"\"\n",
    "        # Project embeddings\n",
    "        z1 = self.projection_head(embeddings1)\n",
    "        z2 = self.projection_head(embeddings2)\n",
    "        \n",
    "        # Normalize\n",
    "        z1 = F.normalize(z1, dim=1)\n",
    "        z2 = F.normalize(z2, dim=1)\n",
    "        \n",
    "        # Compute similarity matrix\n",
    "        similarity_matrix = torch.matmul(z1, z2.T) / temperature\n",
    "        \n",
    "        # Labels (diagonal should be positive pairs)\n",
    "        labels = torch.arange(z1.size(0), device=z1.device)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        return loss\n",
    "\n",
    "class LogGraphSSL(nn.Module):\n",
    "    \"\"\"Complete LogGraph-SSL model with self-supervised learning\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim=128, hidden_dim=256, \n",
    "                 encoder_type='gcn', num_layers=3, dropout=0.1, heads=4):\n",
    "        super(LogGraphSSL, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder_type = encoder_type\n",
    "        \n",
    "        # Choose encoder architecture\n",
    "        if encoder_type == 'gcn':\n",
    "            self.encoder = GCNEncoder(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "        elif encoder_type == 'gat':\n",
    "            self.encoder = GATEncoder(vocab_size, embedding_dim, hidden_dim, num_layers, heads, dropout)\n",
    "        elif encoder_type == 'sage':\n",
    "            self.encoder = GraphSAGEEncoder(vocab_size, embedding_dim, hidden_dim, num_layers, dropout)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown encoder type: {encoder_type}\")\n",
    "        \n",
    "        # Task heads\n",
    "        self.anomaly_head = AnomalyDetectionHead(hidden_dim)\n",
    "        self.ssl_manager = SSLTaskManager(hidden_dim, vocab_size)\n",
    "        \n",
    "        # Training mode flags\n",
    "        self.ssl_training = True\n",
    "        \n",
    "    def forward(self, x, edge_index, batch=None):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Get node embeddings from encoder\n",
    "        node_embeddings = self.encoder(x, edge_index, batch)\n",
    "        \n",
    "        # Anomaly detection logits\n",
    "        anomaly_logits = self.anomaly_head(node_embeddings, batch)\n",
    "        \n",
    "        return {\n",
    "            'node_embeddings': node_embeddings,\n",
    "            'anomaly_logits': anomaly_logits\n",
    "        }\n",
    "    \n",
    "    def ssl_forward(self, x, edge_index, batch=None):\n",
    "        \"\"\"Forward pass for SSL training\"\"\"\n",
    "        outputs = self.forward(x, edge_index, batch)\n",
    "        outputs['ssl_manager'] = self.ssl_manager\n",
    "        return outputs\n",
    "\n",
    "print(\"‚úÖ Model architectures defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef72222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading and Model Configuration\n",
    "\n",
    "def load_hdfs_data():\n",
    "    \"\"\"Load and process HDFS data\"\"\"\n",
    "    print(\"üìÇ Loading HDFS data...\")\n",
    "    \n",
    "    # Define data files\n",
    "    data_files = {\n",
    "        'hdfs_full.txt': 'hdfs_full_labels.txt',\n",
    "        'hdfs_train.txt': 'hdfs_train_labels.txt',  \n",
    "        'hdfs_test.txt': 'hdfs_test_labels.txt'\n",
    "    }\n",
    "    \n",
    "    # Try to find available data files\n",
    "    available_file = None\n",
    "    for log_file, label_file in data_files.items():\n",
    "        if os.path.exists(log_file):\n",
    "            if os.path.exists(label_file):\n",
    "                print(f\"‚úÖ Found {log_file} with labels {label_file}\")\n",
    "                available_file = (log_file, label_file)\n",
    "                break\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è  Found {log_file} but no labels {label_file}\")\n",
    "                available_file = (log_file, None)\n",
    "    \n",
    "    if available_file is None:\n",
    "        print(\"‚ùå No HDFS data files found!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load data using processor\n",
    "    log_file, label_file = available_file\n",
    "    graphs, labels = data_processor.load_hdfs_data(log_file, label_file)\n",
    "    \n",
    "    # Split data\n",
    "    if len(graphs) > 1000:  # Only split if we have enough data\n",
    "        train_graphs, test_graphs, train_labels, test_labels = train_test_split(\n",
    "            graphs, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "        )\n",
    "        print(f\"üìä Train: {len(train_graphs)}, Test: {len(test_graphs)}\")\n",
    "    else:\n",
    "        train_graphs, train_labels = graphs, labels\n",
    "        test_graphs, test_labels = graphs[-100:], labels[-100:]  # Use last 100 for testing\n",
    "        print(f\"üìä Small dataset - Train: {len(train_graphs)}, Test: {len(test_graphs)}\")\n",
    "    \n",
    "    return (train_graphs, train_labels), (test_graphs, test_labels)\n",
    "\n",
    "def create_model_config():\n",
    "    \"\"\"Create model configuration\"\"\"\n",
    "    config = {\n",
    "        'vocab_size': len(data_processor.token_to_id),\n",
    "        'embedding_dim': 128,\n",
    "        'hidden_dim': 256,\n",
    "        'encoder_type': 'gcn',  # Options: 'gcn', 'gat', 'sage'\n",
    "        'num_layers': 3,\n",
    "        'dropout': 0.1,\n",
    "        'heads': 4,  # For GAT\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 1e-5,\n",
    "        'batch_size': 32,\n",
    "        'ssl_epochs': 50,\n",
    "        'supervised_epochs': 30,\n",
    "        'ssl_weight': 1.0,\n",
    "        'supervised_weight': 1.0\n",
    "    }\n",
    "    return config\n",
    "\n",
    "def create_model(config):\n",
    "    \"\"\"Create and initialize model\"\"\"\n",
    "    print(f\"üèóÔ∏è Creating {config['encoder_type'].upper()} model...\")\n",
    "    \n",
    "    model = LogGraphSSL(\n",
    "        vocab_size=config['vocab_size'],\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        encoder_type=config['encoder_type'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        heads=config['heads']\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"üîß Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "class GraphDataLoader:\n",
    "    \"\"\"Custom data loader for graph data\"\"\"\n",
    "    def __init__(self, graphs, labels, batch_size=32, shuffle=True):\n",
    "        self.graphs = graphs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = list(range(len(graphs)))\n",
    "        \n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "        \n",
    "        for i in range(0, len(self.indices), self.batch_size):\n",
    "            batch_indices = self.indices[i:i + self.batch_size]\n",
    "            batch_graphs = [self.graphs[idx] for idx in batch_indices]\n",
    "            batch_labels = [self.labels[idx] for idx in batch_indices]\n",
    "            \n",
    "            # Create batch\n",
    "            try:\n",
    "                batch = Batch.from_data_list(batch_graphs)\n",
    "                batch_labels = torch.tensor(batch_labels, dtype=torch.long)\n",
    "                yield batch.to(device), batch_labels.to(device)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Skipping batch due to error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.indices) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "print(\"‚úÖ Data loading and configuration functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af74ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Functions\n",
    "\n",
    "class TrainingManager:\n",
    "    \"\"\"Comprehensive training manager for SSL and supervised learning\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config):\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizers\n",
    "        self.ssl_optimizer = optim.Adam(model.parameters(), \n",
    "                                       lr=config['learning_rate'], \n",
    "                                       weight_decay=config['weight_decay'])\n",
    "        self.supervised_optimizer = optim.Adam(model.parameters(), \n",
    "                                             lr=config['learning_rate'] * 0.1,  # Lower LR for fine-tuning\n",
    "                                             weight_decay=config['weight_decay'])\n",
    "        \n",
    "        # Schedulers\n",
    "        self.ssl_scheduler = ReduceLROnPlateau(self.ssl_optimizer, patience=10, factor=0.5)\n",
    "        self.supervised_scheduler = ReduceLROnPlateau(self.supervised_optimizer, patience=5, factor=0.5)\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.ssl_metrics = {'loss': [], 'node_loss': [], 'edge_loss': [], 'contrastive_loss': []}\n",
    "        self.supervised_metrics = {'loss': [], 'accuracy': [], 'f1': []}\n",
    "        \n",
    "    def generate_negative_edges(self, edge_index, num_nodes, num_neg_samples=None):\n",
    "        \"\"\"Generate negative edges for edge prediction task\"\"\"\n",
    "        if num_neg_samples is None:\n",
    "            num_neg_samples = edge_index.size(1)\n",
    "        \n",
    "        # Get all possible edges\n",
    "        all_edges = set()\n",
    "        for i in range(edge_index.size(1)):\n",
    "            all_edges.add((edge_index[0, i].item(), edge_index[1, i].item()))\n",
    "        \n",
    "        # Generate negative edges\n",
    "        neg_edges = []\n",
    "        attempts = 0\n",
    "        max_attempts = num_neg_samples * 10\n",
    "        \n",
    "        while len(neg_edges) < num_neg_samples and attempts < max_attempts:\n",
    "            src = np.random.randint(0, num_nodes)\n",
    "            dst = np.random.randint(0, num_nodes)\n",
    "            \n",
    "            if src != dst and (src, dst) not in all_edges:\n",
    "                neg_edges.append([src, dst])\n",
    "            attempts += 1\n",
    "        \n",
    "        if len(neg_edges) == 0:\n",
    "            # Fallback: create some random negative edges\n",
    "            neg_edges = [[0, 1]] if num_nodes > 1 else [[0, 0]]\n",
    "        \n",
    "        return torch.tensor(neg_edges, dtype=torch.long, device=self.device).t()\n",
    "    \n",
    "    def mask_nodes(self, x, mask_ratio=0.15):\n",
    "        \"\"\"Mask random nodes for self-supervised learning\"\"\"\n",
    "        if x.dim() == 2 and x.size(1) == 1:\n",
    "            x = x.squeeze(1)\n",
    "        \n",
    "        num_nodes = x.size(0)\n",
    "        num_mask = max(1, int(num_nodes * mask_ratio))\n",
    "        \n",
    "        # Choose random nodes to mask\n",
    "        mask_indices = torch.randperm(num_nodes)[:num_mask]\n",
    "        \n",
    "        # Store original tokens\n",
    "        original_tokens = x.clone()\n",
    "        \n",
    "        # Mask tokens (replace with <UNK> token which is ID 1)\n",
    "        x_masked = x.clone()\n",
    "        x_masked[mask_indices] = 1  # <UNK> token\n",
    "        \n",
    "        return x_masked.unsqueeze(1), original_tokens, mask_indices\n",
    "    \n",
    "    def ssl_train_step(self, batch, batch_labels):\n",
    "        \"\"\"Single SSL training step\"\"\"\n",
    "        self.model.train()\n",
    "        self.ssl_optimizer.zero_grad()\n",
    "        \n",
    "        # Prepare data\n",
    "        x, edge_index = batch.x, batch.edge_index\n",
    "        batch_info = getattr(batch, 'batch', None)\n",
    "        \n",
    "        # Mask nodes for node prediction task\n",
    "        x_masked, original_tokens, mask_indices = self.mask_nodes(x.squeeze(1) if x.dim() == 2 else x)\n",
    "        \n",
    "        # Forward pass with masked input\n",
    "        outputs = self.model.ssl_forward(x_masked, edge_index, batch_info)\n",
    "        node_embeddings = outputs['node_embeddings']\n",
    "        ssl_manager = outputs['ssl_manager']\n",
    "        \n",
    "        # SSL losses\n",
    "        losses = {}\n",
    "        \n",
    "        # Node masking loss\n",
    "        if len(mask_indices) > 0:\n",
    "            losses['node'] = ssl_manager.node_masking_loss(node_embeddings, original_tokens, mask_indices)\n",
    "        else:\n",
    "            losses['node'] = torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        # Edge prediction loss\n",
    "        try:\n",
    "            neg_edge_index = self.generate_negative_edges(edge_index, batch.num_nodes)\n",
    "            losses['edge'] = ssl_manager.edge_prediction_loss(node_embeddings, edge_index, neg_edge_index)\n",
    "        except:\n",
    "            losses['edge'] = torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        # Contrastive loss (using graph-level embeddings)\n",
    "        try:\n",
    "            if batch_info is not None and len(torch.unique(batch_info)) > 1:\n",
    "                # Multiple graphs in batch\n",
    "                graph_embeddings = global_mean_pool(node_embeddings, batch_info)\n",
    "                if graph_embeddings.size(0) > 1:\n",
    "                    # Create augmented views (simple dropout)\n",
    "                    aug_embeddings = F.dropout(graph_embeddings, p=0.1, training=True)\n",
    "                    losses['contrastive'] = ssl_manager.contrastive_loss(graph_embeddings, aug_embeddings)\n",
    "                else:\n",
    "                    losses['contrastive'] = torch.tensor(0.0, device=self.device)\n",
    "            else:\n",
    "                losses['contrastive'] = torch.tensor(0.0, device=self.device)\n",
    "        except:\n",
    "            losses['contrastive'] = torch.tensor(0.0, device=self.device)\n",
    "        \n",
    "        # Total SSL loss\n",
    "        total_loss = losses['node'] + losses['edge'] + 0.1 * losses['contrastive']\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.ssl_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'node_loss': losses['node'].item(),\n",
    "            'edge_loss': losses['edge'].item(),\n",
    "            'contrastive_loss': losses['contrastive'].item()\n",
    "        }\n",
    "    \n",
    "    def supervised_train_step(self, batch, batch_labels):\n",
    "        \"\"\"Single supervised training step\"\"\"\n",
    "        self.model.train()\n",
    "        self.supervised_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(batch.x, batch.edge_index, getattr(batch, 'batch', None))\n",
    "        logits = outputs['anomaly_logits']\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.cross_entropy(logits, batch_labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.supervised_optimizer.step()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        accuracy = (pred == batch_labels).float().mean()\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'accuracy': accuracy.item(),\n",
    "            'predictions': pred.cpu().numpy(),\n",
    "            'labels': batch_labels.cpu().numpy()\n",
    "        }\n",
    "    \n",
    "    def evaluate(self, data_loader):\n",
    "        \"\"\"Evaluate model on data\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch, batch_labels in data_loader:\n",
    "                outputs = self.model(batch.x, batch.edge_index, getattr(batch, 'batch', None))\n",
    "                logits = outputs['anomaly_logits']\n",
    "                \n",
    "                loss = F.cross_entropy(logits, batch_labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                pred = torch.argmax(logits, dim=1)\n",
    "                all_preds.extend(pred.cpu().numpy())\n",
    "                all_labels.extend(batch_labels.cpu().numpy())\n",
    "        \n",
    "        # Compute metrics\n",
    "        accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "        \n",
    "        return {\n",
    "            'loss': total_loss / len(data_loader),\n",
    "            'accuracy': accuracy,\n",
    "            'predictions': all_preds,\n",
    "            'labels': all_labels\n",
    "        }\n",
    "\n",
    "def train_ssl_phase(model, train_loader, config):\n",
    "    \"\"\"Self-supervised pre-training phase\"\"\"\n",
    "    print(\"üéØ Starting Self-Supervised Learning Phase...\")\n",
    "    \n",
    "    trainer = TrainingManager(model, config)\n",
    "    \n",
    "    for epoch in range(config['ssl_epochs']):\n",
    "        epoch_losses = {'total': [], 'node': [], 'edge': [], 'contrastive': []}\n",
    "        \n",
    "        # Training loop\n",
    "        pbar = tqdm(train_loader, desc=f\"SSL Epoch {epoch+1}/{config['ssl_epochs']}\")\n",
    "        for batch, batch_labels in pbar:\n",
    "            try:\n",
    "                losses = trainer.ssl_train_step(batch, batch_labels)\n",
    "                \n",
    "                epoch_losses['total'].append(losses['total_loss'])\n",
    "                epoch_losses['node'].append(losses['node_loss'])\n",
    "                epoch_losses['edge'].append(losses['edge_loss'])\n",
    "                epoch_losses['contrastive'].append(losses['contrastive_loss'])\n",
    "                \n",
    "                # Update progress bar\n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f\"{losses['total_loss']:.4f}\",\n",
    "                    'Node': f\"{losses['node_loss']:.4f}\",\n",
    "                    'Edge': f\"{losses['edge_loss']:.4f}\"\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error in SSL training step: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        avg_loss = np.mean(epoch_losses['total'])\n",
    "        trainer.ssl_scheduler.step(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"üìä SSL Epoch {epoch+1}: \"\n",
    "                  f\"Loss: {avg_loss:.4f}, \"\n",
    "                  f\"Node: {np.mean(epoch_losses['node']):.4f}, \"\n",
    "                  f\"Edge: {np.mean(epoch_losses['edge']):.4f}\")\n",
    "    \n",
    "    print(\"‚úÖ SSL pre-training completed!\")\n",
    "    return trainer\n",
    "\n",
    "def train_supervised_phase(trainer, train_loader, test_loader, config):\n",
    "    \"\"\"Supervised fine-tuning phase\"\"\"\n",
    "    print(\"üéØ Starting Supervised Fine-tuning Phase...\")\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(config['supervised_epochs']):\n",
    "        # Training\n",
    "        epoch_losses = []\n",
    "        epoch_accuracies = []\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Supervised Epoch {epoch+1}/{config['supervised_epochs']}\")\n",
    "        for batch, batch_labels in pbar:\n",
    "            try:\n",
    "                metrics = trainer.supervised_train_step(batch, batch_labels)\n",
    "                \n",
    "                epoch_losses.append(metrics['loss'])\n",
    "                epoch_accuracies.append(metrics['accuracy'])\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'Loss': f\"{metrics['loss']:.4f}\",\n",
    "                    'Acc': f\"{metrics['accuracy']:.4f}\"\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error in supervised training step: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Evaluation\n",
    "        train_metrics = {\n",
    "            'loss': np.mean(epoch_losses),\n",
    "            'accuracy': np.mean(epoch_accuracies)\n",
    "        }\n",
    "        \n",
    "        test_metrics = trainer.evaluate(test_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        trainer.supervised_scheduler.step(test_metrics['loss'])\n",
    "        \n",
    "        # Save best model\n",
    "        if test_metrics['accuracy'] > best_accuracy:\n",
    "            best_accuracy = test_metrics['accuracy']\n",
    "            best_model_state = trainer.model.state_dict().copy()\n",
    "        \n",
    "        # Log metrics\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"üìä Supervised Epoch {epoch+1}: \"\n",
    "                  f\"Train Loss: {train_metrics['loss']:.4f}, \"\n",
    "                  f\"Train Acc: {train_metrics['accuracy']:.4f}, \"\n",
    "                  f\"Test Loss: {test_metrics['loss']:.4f}, \"\n",
    "                  f\"Test Acc: {test_metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        trainer.model.load_state_dict(best_model_state)\n",
    "        print(f\"‚úÖ Loaded best model with accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    print(\"‚úÖ Supervised fine-tuning completed!\")\n",
    "    return trainer, best_accuracy\n",
    "\n",
    "print(\"‚úÖ Training functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c701934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Training Execution\n",
    "\n",
    "def main_training_pipeline():\n",
    "    \"\"\"Complete training pipeline execution\"\"\"\n",
    "    print(\"üöÄ Starting LogGraph-SSL Training Pipeline...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"üî• GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Load and process data\n",
    "        print(\"\\nüìÇ Step 1: Loading and processing data...\")\n",
    "        train_data, test_data = load_hdfs_data()\n",
    "        \n",
    "        if train_data is None:\n",
    "            print(\"‚ùå Failed to load data. Please check data files.\")\n",
    "            return None\n",
    "        \n",
    "        train_graphs, train_labels = train_data\n",
    "        test_graphs, test_labels = test_data\n",
    "        \n",
    "        print(f\"‚úÖ Data loaded successfully!\")\n",
    "        print(f\"üìä Training samples: {len(train_graphs)}\")\n",
    "        print(f\"üìä Test samples: {len(test_graphs)}\")\n",
    "        print(f\"üìä Vocabulary size: {len(data_processor.token_to_id)}\")\n",
    "        \n",
    "        # Check label distribution\n",
    "        unique_labels = np.unique(train_labels)\n",
    "        label_counts = Counter(train_labels)\n",
    "        print(f\"üìà Label distribution: {dict(label_counts)}\")\n",
    "        \n",
    "        # Step 2: Create model configuration\n",
    "        print(\"\\n‚öôÔ∏è Step 2: Creating model configuration...\")\n",
    "        config = create_model_config()\n",
    "        print(\"‚úÖ Configuration created:\")\n",
    "        for key, value in config.items():\n",
    "            print(f\"   {key}: {value}\")\n",
    "        \n",
    "        # Step 3: Initialize model\n",
    "        print(f\"\\nüèóÔ∏è Step 3: Initializing {config['encoder_type'].upper()} model...\")\n",
    "        model = create_model(config)\n",
    "        print(\"‚úÖ Model initialized successfully!\")\n",
    "        \n",
    "        # Step 4: Create data loaders\n",
    "        print(\"\\nüì¶ Step 4: Creating data loaders...\")\n",
    "        train_loader = GraphDataLoader(train_graphs, train_labels, \n",
    "                                     config['batch_size'], shuffle=True)\n",
    "        test_loader = GraphDataLoader(test_graphs, test_labels, \n",
    "                                    config['batch_size'], shuffle=False)\n",
    "        \n",
    "        print(f\"‚úÖ Data loaders created:\")\n",
    "        print(f\"   Training batches: {len(train_loader)}\")\n",
    "        print(f\"   Test batches: {len(test_loader)}\")\n",
    "        \n",
    "        # Step 5: Self-supervised pre-training\n",
    "        print(f\"\\nüéØ Step 5: Self-supervised pre-training ({config['ssl_epochs']} epochs)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        trainer = train_ssl_phase(model, train_loader, config)\n",
    "        \n",
    "        ssl_time = time.time() - start_time\n",
    "        print(f\"‚è±Ô∏è SSL training completed in {ssl_time/60:.1f} minutes\")\n",
    "        \n",
    "        # Step 6: Supervised fine-tuning\n",
    "        print(f\"\\nüéØ Step 6: Supervised fine-tuning ({config['supervised_epochs']} epochs)...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        trainer, best_accuracy = train_supervised_phase(trainer, train_loader, test_loader, config)\n",
    "        \n",
    "        supervised_time = time.time() - start_time\n",
    "        print(f\"‚è±Ô∏è Supervised training completed in {supervised_time/60:.1f} minutes\")\n",
    "        \n",
    "        # Step 7: Final evaluation\n",
    "        print(\"\\nüìä Step 7: Final comprehensive evaluation...\")\n",
    "        final_test_metrics = trainer.evaluate(test_loader)\n",
    "        \n",
    "        print(\"üéâ Training Pipeline Completed Successfully!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"üìà Final Results:\")\n",
    "        print(f\"   Test Accuracy: {final_test_metrics['accuracy']:.4f}\")\n",
    "        print(f\"   Test Loss: {final_test_metrics['loss']:.4f}\")\n",
    "        print(f\"   Best Accuracy: {best_accuracy:.4f}\")\n",
    "        print(f\"   Total Training Time: {(ssl_time + supervised_time)/60:.1f} minutes\")\n",
    "        \n",
    "        # Classification report\n",
    "        if len(np.unique(final_test_metrics['labels'])) > 1:\n",
    "            from sklearn.metrics import classification_report\n",
    "            print(\"\\nüìã Classification Report:\")\n",
    "            print(classification_report(final_test_metrics['labels'], \n",
    "                                       final_test_metrics['predictions'],\n",
    "                                       target_names=['Normal', 'Anomaly']))\n",
    "        \n",
    "        # Save model\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_path = f\"loggraph_ssl_model_{timestamp}.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config,\n",
    "            'vocab': data_processor.token_to_id,\n",
    "            'test_accuracy': final_test_metrics['accuracy']\n",
    "        }, model_path)\n",
    "        print(f\"üíæ Model saved to: {model_path}\")\n",
    "        \n",
    "        return {\n",
    "            'model': model,\n",
    "            'trainer': trainer,\n",
    "            'config': config,\n",
    "            'final_metrics': final_test_metrics,\n",
    "            'best_accuracy': best_accuracy\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in training pipeline: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Execute the training pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Executing LogGraph-SSL Training Pipeline...\")\n",
    "    results = main_training_pipeline()\n",
    "    \n",
    "    if results is not None:\n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        print(f\"üéØ Final test accuracy: {results['final_metrics']['accuracy']:.4f}\")\n",
    "    else:\n",
    "        print(\"‚ùå Training failed. Please check the logs above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ece7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring and Visualization\n",
    "\n",
    "def plot_training_metrics(trainer, save_path=None):\n",
    "    \"\"\"Plot training metrics and GPU utilization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # SSL losses\n",
    "    if trainer.ssl_metrics['loss']:\n",
    "        axes[0, 0].plot(trainer.ssl_metrics['loss'], label='Total Loss')\n",
    "        axes[0, 0].plot(trainer.ssl_metrics['node_loss'], label='Node Loss')\n",
    "        axes[0, 0].plot(trainer.ssl_metrics['edge_loss'], label='Edge Loss')\n",
    "        axes[0, 0].set_title('Self-Supervised Learning Losses')\n",
    "        axes[0, 0].set_xlabel('Epoch')\n",
    "        axes[0, 0].set_ylabel('Loss')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True)\n",
    "    \n",
    "    # Supervised metrics\n",
    "    if trainer.supervised_metrics['loss']:\n",
    "        axes[0, 1].plot(trainer.supervised_metrics['loss'], label='Loss', color='red')\n",
    "        axes[0, 1].set_title('Supervised Learning Loss')\n",
    "        axes[0, 1].set_xlabel('Epoch')\n",
    "        axes[0, 1].set_ylabel('Loss')\n",
    "        axes[0, 1].grid(True)\n",
    "        \n",
    "        # Accuracy on secondary y-axis\n",
    "        ax2 = axes[0, 1].twinx()\n",
    "        ax2.plot(trainer.supervised_metrics['accuracy'], label='Accuracy', color='blue')\n",
    "        ax2.set_ylabel('Accuracy')\n",
    "        ax2.legend(loc='upper right')\n",
    "    \n",
    "    # GPU memory usage (if available)\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        \n",
    "        axes[1, 0].bar(['Allocated', 'Reserved', 'Total'], \n",
    "                      [memory_allocated, memory_reserved, total_memory],\n",
    "                      color=['orange', 'red', 'blue'])\n",
    "        axes[1, 0].set_title('GPU Memory Usage (GB)')\n",
    "        axes[1, 0].set_ylabel('Memory (GB)')\n",
    "    \n",
    "    # Model architecture summary\n",
    "    if hasattr(trainer, 'model'):\n",
    "        total_params = sum(p.numel() for p in trainer.model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)\n",
    "        \n",
    "        axes[1, 1].bar(['Total', 'Trainable'], \n",
    "                      [total_params, trainable_params],\n",
    "                      color=['lightblue', 'darkblue'])\n",
    "        axes[1, 1].set_title('Model Parameters')\n",
    "        axes[1, 1].set_ylabel('Number of Parameters')\n",
    "        axes[1, 1].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"üìä Training plots saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def monitor_gpu_usage():\n",
    "    \"\"\"Monitor and display current GPU usage\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"‚ö†Ô∏è CUDA not available\")\n",
    "        return\n",
    "    \n",
    "    print(\"üî• GPU Monitoring:\")\n",
    "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"   Memory Reserved: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "    print(f\"   Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Memory utilization percentage\n",
    "    memory_percent = (torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory) * 100\n",
    "    print(f\"   Utilization: {memory_percent:.1f}%\")\n",
    "    \n",
    "    # Temperature and power (if available)\n",
    "    try:\n",
    "        import subprocess\n",
    "        result = subprocess.run(['nvidia-smi', '--query-gpu=temperature.gpu,power.draw', \n",
    "                               '--format=csv,noheader,nounits'], \n",
    "                              capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            temp, power = result.stdout.strip().split(', ')\n",
    "            print(f\"   Temperature: {temp}¬∞C\")\n",
    "            print(f\"   Power Draw: {power}W\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def create_training_dashboard(trainer=None):\n",
    "    \"\"\"Create a simple training dashboard\"\"\"\n",
    "    print(\"üìä LogGraph-SSL Training Dashboard\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # System information\n",
    "    print(\"üñ•Ô∏è System Information:\")\n",
    "    print(f\"   Python Version: {sys.version.split()[0]}\")\n",
    "    print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    \n",
    "    # Memory usage\n",
    "    print(f\"\\nüíæ Memory Usage:\")\n",
    "    print(f\"   RAM: {psutil.virtual_memory().percent:.1f}% used\")\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        memory_percent = (torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory) * 100\n",
    "        print(f\"   GPU: {memory_percent:.1f}% used\")\n",
    "    \n",
    "    # Training status\n",
    "    if trainer is not None:\n",
    "        print(f\"\\nüéØ Training Status:\")\n",
    "        print(f\"   SSL Epochs Completed: {len(trainer.ssl_metrics['loss'])}\")\n",
    "        print(f\"   Supervised Epochs Completed: {len(trainer.supervised_metrics['loss'])}\")\n",
    "        \n",
    "        if trainer.ssl_metrics['loss']:\n",
    "            print(f\"   Latest SSL Loss: {trainer.ssl_metrics['loss'][-1]:.4f}\")\n",
    "        \n",
    "        if trainer.supervised_metrics['accuracy']:\n",
    "            print(f\"   Latest Accuracy: {trainer.supervised_metrics['accuracy'][-1]:.4f}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Test GPU and create initial dashboard\n",
    "print(\"üîç Initial System Check:\")\n",
    "monitor_gpu_usage()\n",
    "print(\"\\nüìä Creating training dashboard...\")\n",
    "create_training_dashboard()\n",
    "print(\"‚úÖ Monitoring setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cdfbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick Test and Validation\n",
    "\n",
    "def quick_test_setup():\n",
    "    \"\"\"Quick test to validate everything works before full training\"\"\"\n",
    "    print(\"üß™ Running Quick Setup Validation...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Check data files\n",
    "        print(\"üìÇ Test 1: Checking data files...\")\n",
    "        data_files = ['hdfs_full.txt', 'hdfs_train.txt', 'hdfs_test.txt']\n",
    "        found_files = [f for f in data_files if os.path.exists(f)]\n",
    "        \n",
    "        if found_files:\n",
    "            print(f\"‚úÖ Found data files: {found_files}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No data files found. Creating sample data...\")\n",
    "            # Create minimal sample data for testing\n",
    "            sample_logs = [\n",
    "                \"E1 E2 E3 E4\",\n",
    "                \"E1 E5 E6\",\n",
    "                \"E2 E3 E7 E8\",\n",
    "                \"E1 E2 E9\",\n",
    "                \"E5 E6 E10 E11\"\n",
    "            ]\n",
    "            sample_labels = [0, 1, 0, 1, 0]\n",
    "            \n",
    "            with open('sample_logs.txt', 'w') as f:\n",
    "                for log in sample_logs:\n",
    "                    f.write(log + '\\n')\n",
    "            \n",
    "            with open('sample_labels.txt', 'w') as f:\n",
    "                for label in sample_labels:\n",
    "                    f.write(str(label) + '\\n')\n",
    "            \n",
    "            print(\"‚úÖ Created sample data files\")\n",
    "        \n",
    "        # Test 2: Data processing\n",
    "        print(\"\\nüîß Test 2: Testing data processing...\")\n",
    "        test_processor = HDFSDataProcessor(vocab_size=100)\n",
    "        \n",
    "        # Use sample data or existing data\n",
    "        if os.path.exists('sample_logs.txt'):\n",
    "            test_graphs, test_labels = test_processor.load_hdfs_data('sample_logs.txt', 'sample_labels.txt')\n",
    "        elif found_files:\n",
    "            test_graphs, test_labels = test_processor.load_hdfs_data(found_files[0])\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No data files available\")\n",
    "        \n",
    "        print(f\"‚úÖ Processed {len(test_graphs)} graphs\")\n",
    "        print(f\"   Vocabulary size: {len(test_processor.token_to_id)}\")\n",
    "        \n",
    "        # Test 3: Model creation\n",
    "        print(\"\\nüèóÔ∏è Test 3: Testing model creation...\")\n",
    "        test_config = {\n",
    "            'vocab_size': len(test_processor.token_to_id),\n",
    "            'embedding_dim': 64,\n",
    "            'hidden_dim': 128,\n",
    "            'encoder_type': 'gcn',\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.1,\n",
    "            'heads': 2\n",
    "        }\n",
    "        \n",
    "        test_model = LogGraphSSL(**{k: v for k, v in test_config.items() if k != 'heads' or test_config['encoder_type'] == 'gat'}).to(device)\n",
    "        print(f\"‚úÖ Created {test_config['encoder_type'].upper()} model\")\n",
    "        print(f\"   Parameters: {sum(p.numel() for p in test_model.parameters()):,}\")\n",
    "        \n",
    "        # Test 4: Single forward pass\n",
    "        print(\"\\n‚ö° Test 4: Testing forward pass...\")\n",
    "        if test_graphs:\n",
    "            sample_graph = test_graphs[0].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = test_model(sample_graph.x, sample_graph.edge_index)\n",
    "                print(f\"‚úÖ Forward pass successful\")\n",
    "                print(f\"   Node embeddings shape: {outputs['node_embeddings'].shape}\")\n",
    "                print(f\"   Anomaly logits shape: {outputs['anomaly_logits'].shape}\")\n",
    "        \n",
    "        # Test 5: Data loader\n",
    "        print(\"\\nüì¶ Test 5: Testing data loader...\")\n",
    "        test_loader = GraphDataLoader(test_graphs[:3], test_labels[:3], batch_size=2)\n",
    "        \n",
    "        for batch, labels in test_loader:\n",
    "            print(f\"‚úÖ Data loader working\")\n",
    "            print(f\"   Batch size: {batch.batch.max().item() + 1 if hasattr(batch, 'batch') else 1}\")\n",
    "            print(f\"   Labels shape: {labels.shape}\")\n",
    "            break\n",
    "        \n",
    "        print(\"\\nüéâ All tests passed! Ready for full training.\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "def run_mini_training():\n",
    "    \"\"\"Run a mini training session to test the pipeline\"\"\"\n",
    "    print(\"üèÉ‚Äç‚ôÇÔ∏è Running Mini Training Session...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Load minimal data\n",
    "        test_processor = HDFSDataProcessor(vocab_size=100)\n",
    "        \n",
    "        if os.path.exists('sample_logs.txt'):\n",
    "            graphs, labels = test_processor.load_hdfs_data('sample_logs.txt', 'sample_labels.txt')\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No sample data available. Run quick_test_setup() first.\")\n",
    "            return False\n",
    "        \n",
    "        # Create mini config\n",
    "        mini_config = {\n",
    "            'vocab_size': len(test_processor.token_to_id),\n",
    "            'embedding_dim': 32,\n",
    "            'hidden_dim': 64,\n",
    "            'encoder_type': 'gcn',\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.1,\n",
    "            'heads': 2,\n",
    "            'learning_rate': 0.01,\n",
    "            'weight_decay': 1e-4,\n",
    "            'batch_size': 2,\n",
    "            'ssl_epochs': 3,\n",
    "            'supervised_epochs': 2\n",
    "        }\n",
    "        \n",
    "        # Create model and trainer\n",
    "        model = LogGraphSSL(\n",
    "            vocab_size=mini_config['vocab_size'],\n",
    "            embedding_dim=mini_config['embedding_dim'],\n",
    "            hidden_dim=mini_config['hidden_dim'],\n",
    "            encoder_type=mini_config['encoder_type'],\n",
    "            num_layers=mini_config['num_layers'],\n",
    "            dropout=mini_config['dropout']\n",
    "        ).to(device)\n",
    "        \n",
    "        # Create data loader\n",
    "        train_loader = GraphDataLoader(graphs, labels, mini_config['batch_size'])\n",
    "        \n",
    "        # Quick SSL training\n",
    "        print(\"üéØ Mini SSL training...\")\n",
    "        trainer = TrainingManager(model, mini_config)\n",
    "        \n",
    "        for epoch in range(mini_config['ssl_epochs']):\n",
    "            for batch, batch_labels in train_loader:\n",
    "                losses = trainer.ssl_train_step(batch, batch_labels)\n",
    "                print(f\"   SSL Epoch {epoch+1}, Loss: {losses['total_loss']:.4f}\")\n",
    "                break  # One batch per epoch for speed\n",
    "        \n",
    "        # Quick supervised training\n",
    "        print(\"üéØ Mini supervised training...\")\n",
    "        for epoch in range(mini_config['supervised_epochs']):\n",
    "            for batch, batch_labels in train_loader:\n",
    "                metrics = trainer.supervised_train_step(batch, batch_labels)\n",
    "                print(f\"   Supervised Epoch {epoch+1}, Loss: {metrics['loss']:.4f}, Acc: {metrics['accuracy']:.4f}\")\n",
    "                break  # One batch per epoch for speed\n",
    "        \n",
    "        print(\"‚úÖ Mini training completed successfully!\")\n",
    "        print(\"-\" * 40)\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Mini training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "# Run validation tests\n",
    "print(\"üöÄ Starting Local Validation...\")\n",
    "if quick_test_setup():\n",
    "    print(\"\\nüèÉ‚Äç‚ôÇÔ∏è Running mini training test...\")\n",
    "    run_mini_training()\n",
    "    print(\"\\n‚úÖ Local validation complete! Ready for full training on Jupyter server.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Validation failed. Please fix issues before proceeding.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
